\def\year{2021}\relax
%File: formatting-instructions-latex-2021.tex
%release 2021.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai21}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS

% ADJUSTED BY AFM ----------------------------------------------------------
\usepackage{amsmath} % ENTERED BY AFM
\usepackage{subcaption}
\usepackage[table,usenames,dvipsnames]{xcolor}



\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
\pdfinfo{
/Title (ML Assignment 4)
/Author (Anthony Menninger)
/TemplateVersion (2021.1)
} %Leave this
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai21.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash


%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{
Machine Learning - CS 7641
Assignment 4
	
}
\author {
    % Author
    Anthony Menninger \\
}

\affiliations{
    Georgia Tech OMSCS Program \\
    amenninger3\\
    tmenninger@gatech.edu

}

\begin{document}

\maketitle

\begin{abstract}
This paper explores Markov Decision Processes through the use of Value Iteration, Policy Iteration and Q Learning.  It looks at a simple "Forest" MDP and a larger Frozen Lake "Grid World".
\end{abstract}

\section{Problem Introduction}

A key element in both Markov Decision Processes (MDPs) is the idea of a discount, which is often referred to as \textbf{\emph{Gamma ($\gamma$)}}.  This discount factor, between 1 and 0, allows solutions for infinite MDPs by discounting future value by \textbf{\emph{Gamma}} for each step into the future.  In an infinite activity, this ends up valuing future value at $ \frac{1}{1- \gamma}$.  Gamma close to 0 creates a very close horizon where only immediate rewards are maximized, while Gamma close to 1 creates a long horizon, where maximizing long term reward is emphasized.  As will be seen, MDPs are very sensitive to this and small changes can create very different solutions.

The key toolset I used was the hiive fork [3] of the MDP Toolbox for Python [2], which is derived from the MDP Toolbox [1].  I also used the OpenAI Gym Frozen Lake environment [4] for the larger Frozen Lake MDP.

\subsection{Forest MDP}
\textbf{\emph{Forest MDP}} is a simple MDP from the hiive MDP Toolbox [2] that models the value of a forest with respect to two actions that can be performed each year: \textbf{\emph{wait}} or \textbf{\emph{cut}}.  There is a stochastic element of forest fires that occur with probability \textbf{\emph{p}}.  Each year is a state, with a max of \textbf{\emph{S}} years / states.  Cutting deterministically transitions to the initial state,  \textbf{\emph{state=0}} and provides a \textbf{\emph{cutting reward}}.  Waiting transitions to the next year, or if in the max state, remains there.  Forest fires provide a stochastic element, with a fire transitioning back to the initial state.  A \textbf{\emph{waiting reward}} only occurs in the max state, \textbf{\emph{state=S}}.  This is a continual MDP, with no terminal or absorbing states. 

The base setup for \textbf{\emph{Forest}} is seven years (\textbf{\emph{S=7}}), a 10\% chance of forest fire (\textbf{\emph{p=0.1}}), a  (\textbf{\emph{cutting reward=2}}) in any state and a (\textbf{\emph{waiting reward = 4}}) only in the final state.

In the context of MDP's, each state has an action that maximizes expected value and the set of maximizing actions for all states is called a \textbf{\emph{policy}}.  This policy can be examined to determine what rational actors are likely going to do. 

There are several very interesting aspects to this problem.  \textbf{\emph{Forest MDP}} models a key choice being made today around ecology and the environment.  What are the rewards needed to keep forests around?  How do maximizing actions (\textbf{\emph{policy}}) change as the chance of forest fires increase?  How does the length of the considered horizon (\textbf{\emph{Gamma discount}}) influence expected outcomes.  

This also highlights a key challenge in MDP's and Reinforcement Learning, which is understanding rewards and setting them appropriately.  \textbf{\emph{Forest MDP}}  allows modeling of different situations to inform the construction of public policy for governments, NGO's and corporations.  The \textbf{\emph{cutting reward}} might be clearly modeled using expected timber value, but other types of reward may want to be considered in the \textbf{\emph{waiting reward}}, such as carbon reduction and maintenance of ecological diversity.  This MDP can be used to model economic rewards and penalties to compel desired outcomes, such as a carbon tax that would reduce the value of the \textbf{\emph{cutting reward}}.

As described, this MDP is more likely to be "solved" using Value Iteration (VI) or Policy Iteration (PI) in order to use it for comparing different tradeoffs and outcomes.

\subsection{Lake MDP}
\textbf{\emph{Lake MDP}} is a larger MDP that uses the OpenAI gym Frozen Lake [4,5] to create its environment.   It is a 2D grid with four actions to move \textbf{\emph{up, down, left, right}} and can be of \textbf{\emph{Size}}, which is the number of grids on one size of the square, creating \textbf{\emph{Size x Size}} states.  The lake can be set to not slippery, where the action is deterministic, or slippery, where the action is stochastic.  When slippery, the action has a 1/3 chance of moving in the desired direction, a 1/3 to the left of the desired direction and 1/3 to the right of the desired direction, which is fairly stochastic.  This is an episodic MDP where the player starts in the first state - (0,0) upper left corner.  The goal state (size, size) is in the bottom right corner and has a reward of 1.  There are also random holes which are absorbing states with a reward of 0.

I created Small (\textbf{\emph{Size=4}}), Medium (\textbf{\emph{Size=8}}), and Large (\textbf{\emph{Size=16}}) worlds to explore the impacts of different size worlds.  

This MDP is interesting because it embodies a core Reinforcement Learning challenge of exploring an environment and learning what to do.  It describes a common, everyday task of exploring an environment and learning where the rewards and traps are.  Robots are often in a situation where they need to move about and find out what they should be doing.  In addition, it creates a much more interesting set of transitions between the different states, as compared with the \textbf{\emph{Forest MDP}}, where transitions are either next state or back to beginning.  Also, \textbf{\emph{Lake MDP}} is episodic vs  \textbf{\emph{Forest MDP}} which is continual.

As described, this MDP is more likely to be "learned" in a setting using Q Learning to explore how agents use reinforcement learning to learn to navigate and behave in an unknown environment.

\section{Value Iteration (VI) and Policy Iteration (PI)}
In this section, we start with the \textbf{\emph{Forest MDP}}.   The VI and PI implementations used for this experiment started with the hiive MDP Toolbox [3], which was then "forked" to allow for modifications for this analysis.  Both VI and PI are model based algorithms, which means that the complete MDP is known and used for solving the problem.  This matches the \textbf{\emph{Forest MDP}} because it likely to be used to model different situations to see the impacts of different parameters.   

Both VI an PI rely on the Bellman Equation, which can be represented in different ways as seen in  \textbf{Equation \ref{eq:bellman}} 
\begin{equation} 
\label{eq:bellman}
\begin{gathered}
V(s) = R(s) + \gamma \; max_{a^\prime}  \sum_{s^\prime} T(s,a,s^\prime) V(s^\prime)\\
\hat{Q}(s,a) \xleftarrow{\alpha_t} R(s) + \gamma \;  max_{a^\prime} Q(s^\prime, a^\prime)
\end{gathered}
\end{equation}

With Value Iteration, a sweep is performed of all states to update the Values using \textbf{Equation \ref{eq:bellman}}.  If this is run to infinity, it will converge to the exact solution.  Practically, a setting of acceptable error is created to determine convergence, with each iteration comparing the old values and new values to determine error.  I will refer to this as the \emph{Error Threshold}, which is sometimes confusingly also referred to as $\epsilon$, but we will save that designation for exploration in Q Learning. 

\textbf{Figure \ref{fig:forest_vi}} shows the VI solution for the default setup of the  \textbf{\emph{Lake MDP}} outlined in the introduction with the default VI setting of \emph{Error Threshold=0.01}.  First, we can see that it takes 49 iterations to solve.  Initially, the values for all states are low, but over time the values increase and shift from the high value end state towards the initial state.  As the values increase, the policy changes from cutting in almost all states to waiting in all states.  The final policy is reached in 7 iterations, but the final emph{Error Threshold} values aren't reached for much later.  Looking at the value of the final state, even though we only get a reward of 4 for waiting, our expected value in that state is almost 27 because with this policy we will stay there until a forest fire forces a transition. 





\begin{figure}[!htb]
\centering
\includegraphics[width=2.75in, height=2.75in]{Figures/Forest_VI_Gamma_0_9_Value_vs_Iteration.png}
\caption{Forest MDP Value Iteration (VI) with \emph{Error Threshold=0.01}. Red X indicates cut, otherwise the action is wait. }
\label{fig:forest_vi}
\end{figure}


\begin{figure}[!htb]
\centering
\includegraphics[width=2.75in, height=2.75in]{Figures/Forest_PI_Gamma_0_9_Value_vs_Iteration.png}
\caption{Forest MDP Policy Iteration (PI) }
\label{fig:forest_pi}
\end{figure}

PI starts with an initial setup, in this case with all state values set to 0 and default policy actions.  It then solves for the policy using a linear solver and the initial Values.  With this new policy, it then recalculates all V values using the Bellman Equation \textbf{\ref{value_bellman}}.  This implementation of PI uses the Q update rule in \textbf{Equation \ref{q_bellman}}, and then derives the policy and values from that as seen in  \textbf{Equations \ref{value_equation} and \ref{policy_equation}}.  



\begin{equation} \label{eq:value_policy}
\begin{gathered}
Value(s) = max_a Q(s,a)\\
Policy(s) = argmax_a Q(s,a)
\end{gathered}
\end{equation}

\section{Forest MDP Q Learning}

\section{Lake MDP Q Learning}

\section{Summary}

\section{References}
\begin{tabular}{l p{2.75in}}
\\
1 & Markov Decision Processes (MDP) Toolbox, https://miat.inrae.fr/MDPtoolbox/ Accessed: 11/1/2022.
\\
2 & Markov Decision Process (MDP) Toolbox for Python, https://github.com/sawcordwell/pymdptoolbox: Accessed: 11/1/2022.
\\
3 & hiive Fork: Markov Decision Process (MDP) Toolbox for Python, https://github.com/hiive/hiivemdptoolbox: Accessed: 11/1/2022.
\\
4 & Brockman, G. et al., 2016. Openai gym.
\\
5 & Openai gym Frozen Lake Documentation,  https://www.gymlibrary.dev/environments/toy\_text/frozen\_lake/: Accessed: 11/1/2022.

\end{tabular}
\end{document}
