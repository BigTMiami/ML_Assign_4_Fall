Office Hours 11 10-11-22

Assignment 4

Choose 2 MDPs
Explain MDP's, structure of state space, actions, rewards, transitions
How different?  Intersting properties
Hypothesis structure on challenging, easier for algorithms, what is interesting
Model free algotihm
One non grid world

VI and PI
convergence - artifacts, plot for convergence, can be multiple types
    reward vs iteration (Like fitness vs iteration assign 2)
    delta vs iteration (vs optimal, vs reward change)
tables, visuals
    Policy that was found
    discuss compare \ contrast different algorithms
    try different state space problems - at least small and large
        leads to different behavior, more compelling analysis
    
    how does performance change with different state sizes

RL - can use model free (Q) very different from VI \ PI
Same as VI \ PI section
    convergence
    policies found
    compare \ contrast across algorithms
    * Discuss exploration strategies
    * Hyperparameters on decay, epsilon, 

Towards end
    Compare and contrast 
    Global analysis
    Highlight differences in different algorithms

Other
    Creative
        reward vs time
        state space vs time
    
    exploration \ explotation tradeoff

    mathematical intuition behind it

    discount factor exploration

Discussion
    Convergence - reward convergences

        In Q - Q values not changing

        Episode - reach goal state, made of iteration
            could do reward vs episode

    Should be stochastic
        
Use stochasitic environment, not static
